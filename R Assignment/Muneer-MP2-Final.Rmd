---
title: "Sheikh M Muneer-MP2-Final"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Importing libraries:
```{r}
library(dplyr)
library(tidyverse)
library(openintro)
library(UsingR)
library(statsr)
library(broom)
library(MASS)
library(car)
library(olsrr)
library(ggplot2)
```

## Import Training Dataset:
```{r}
df_ames <- readr::read_csv("train.csv")
head(df_ames)
```

## Import Test Dataset:
```{r}
df_test <- readr::read_csv("test.csv")
head(df_test)
```

## Data Types of DataSet:
```{r}
str(df_ames)
```

## Dimension of DataSet:
```{r}
dim(df_ames)
```
There are 1460 Rows and 81 Columns

## Summary of DataSet:
```{r}
summary(df_ames)
```

# Univariate Analysis:
```{r}
class(df_ames$MSSubClass)
unique(df_ames$MSSubClass)
df_ames$MSSubClass <- as.character(df_ames$MSSubClass)
class(df_ames$MSSubClass)
```
As MSSubClass respresentation is of Categorical so We will convert this into Character Representation.

# Data Cleaning and Imputation:
We are Observing How Manny Columns have Null Values in Specific Columns.
```{r}
All_nulls  <- colSums(is.na(df_ames))
All_nulls  <- All_nulls[All_nulls>0] 
barplot(All_nulls, main="Null Values Comparison in Ames Housing",xlab="Columns",ylab="Number of Missing Values",las=1,col=8)
```

## Mode Function:
We are making Mode Function to Impute Mode on Categorical Columns where values are Nulls.
```{r}
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
```

Dealing of Missing numeric values with Mean Imputation
```{r}
ames_train_int <- dplyr::select_if(df_ames, is.numeric)
ames_train_int
```

Mean Imputation for Numeric Values.
```{r}
ames_train_int[ames_train_int=="NA"] <-  lapply(ames_train_int, mean, na.rm = TRUE) 
ames_train_int[ames_train_int==""] <-    lapply(ames_train_int, mean, na.rm = TRUE)
dim(ames_train_int)
```

Dealing of Missing categorical values  with Mode Imputation for Categorical Variables.
```{r}
ames_train_fac <- dplyr::select_if(df_ames, is_character)
ames_train_fac <- sapply(ames_train_fac, as.character)
ames_train_fac[is.na(ames_train_fac)] <-   getmode(ames_train_fac)  
ames_train_fac[ames_train_fac==""]<- getmode(ames_train_fac)        
ames_train_fac <- data.frame(ames_train_fac)
dim(ames_train_fac)
```

Merging both Numerical and Categoricals Variables in Training DataFrame
```{r}
df_ames <- cbind(ames_train_int, ames_train_fac)
head(df_ames)
```

# Exploratory Data Analysis:

View Sale Price Distribution.
```{r}
hist(df_ames$SalePrice,main="Sales Price Distribution",col=8)
```

Since Sale Price is not linearly Distributed we will use log to Normally Distribute this as Linear Model will Work Perfect on Linear Distribution.
```{r}
hist(log(df_ames$SalePrice),main="Sales Price Normal Distribution",col=8)
```

Analysis of Sale Price by Sale Condition
```{r}
ggplot(df_ames,aes(SaleCondition,log(SalePrice))) +  geom_boxplot(aes(col='Red')) 
```
We can See that Partial Sales Condition have High Sale Price.Normal and Abnormal have Outliers values.

Analysis of Sale Price by Sale Type.
```{r}
ggplot(df_ames,aes(SaleType,log(SalePrice))) +  geom_boxplot(aes(col='Brown')) 
```
We can See that New Sale Type have high Sales Price than Others COD and WD have outliers values.


Analysis of Sale Price by MSSubClass
```{r}
boxplot(df_ames$SalePrice ~ df_ames$MSSubClass,main="Sale Price by MSSubClass",col="Blue")
```

# Linear Regression Modelling :

I will Exclude Utilities Column which have Only 1 level having same Value in All Rows which will not permit our linear model to fit Perfectly.I am going to model Full Linear Model.
```{r}
df_ames = subset(df_ames, select=-c(Utilities))
full_model_lm <- lm(log(SalePrice)~., data=df_ames)
summary(full_model_lm)
```
In Full Model Variability of all Variables to Identify SalePrice is 95% which is Ok.but Residual Standard Error is little bit OK which is 0.09.

Analyze AOV to Identify One Model Variation and Residuals Squared of each Variable
```{r}
aov(full_model_lm)
```
```{r}
par(mfrow=c(2,2))
plot(full_model_lm)
```

Viewing by 360 View:
```{r}
ols_plot_resid_fit(full_model_lm)
ols_plot_resid_lev(full_model_lm)
ols_plot_resid_qq(full_model_lm)
ols_plot_dffits(full_model_lm)
ols_plot_cooksd_chart(full_model_lm)
ols_plot_hadi(full_model_lm)
```

We will make 5 Groups having 5 variables in each group to Idefiy only Significant Variables for our Final Model.

### 1) 1st Group :
```{r}
group_basement <- as.formula(log(SalePrice) ~ BsmtQual+BsmtCond+BsmtFullBath+BsmtFinType1+BsmtFinType2)
pairs(group_basement, data = df_ames)

lm_group_basement <- lm(group_basement, data = df_ames)
summary(lm_group_basement)
```

```{r}
par(mfrow = c(2, 2))
plot(lm_group_basement)
```

Analysis of Variance:
```{r}
aov(lm_group_basement)
```

Detect MultiCollinearity
```{r}
vif(lm_group_basement)
```
We are removing BsmtFinType1 and BsmtFinType2  because Thes variable have high MultiCollinearity.

Model selection using AIC.
```{r}
AIC_lm_group_base <- stepAIC(lm_group_basement,direction = "backward",k=2, trace = 0, steps = 1000)
summary(AIC_lm_group_base)
```
We will chose Most Significant Variables from AIC_lm_group_base Model `(BsmtQual,BsmtCond)`.

### 2) 2nd Group :
```{r}
group_Exter <-   as.formula(log(SalePrice) ~ ExterQual+ExterCond+Exterior1st+Exterior2nd+MasVnrArea)
pairs(group_Exter,data = df_ames)

lm_group_Exter <-   lm(group_Exter, data = df_ames)
summary(lm_group_Exter)
```

```{r}
par(mfrow = c(2, 2))
plot(lm_group_Exter)
```

Analysis of Variance:
```{r}
aov(lm_group_Exter)
```

Detect MultiCollinearity
```{r}
attributes(alias(lm_group_Exter)$Complete)$dimnames[[1]]
```

Model selection using AIC:
```{r}
AIC_lm_group_Exter <- stepAIC(lm_group_Exter,direction = "backward",k=2, trace = 0, steps = 1000)
summary(AIC_lm_group_Exter)
```
We will chose Most Significant Variables from AIC_lm_group_Exter Model `(ExterQual,MasVnrArea)`


### 3) 3rd Group :
```{r}
group_OverAll    <-   as.formula("log(SalePrice) ~ OverallQual+OverallCond+`1stFlrSF`+`2ndFlrSF`+TotalBsmtSF")    
pairs(group_OverAll, data = df_ames)

lm_group_OverAll <-  lm(group_OverAll, data = df_ames)
summary(lm_group_OverAll)
```

```{r}
par(mfrow = c(2, 2))
plot(lm_group_OverAll)
```

Analysis of Variance:
```{r}
aov(lm_group_OverAll)
```

Detect MultiCollinearity:
```{r}
vif(lm_group_OverAll)
```

Model selection using AIC:
```{r}
AIC_lm_group_OverAll <- stepAIC(lm_group_OverAll,direction = "backward",k=2, trace = 0, steps = 1000)
summary(AIC_lm_group_OverAll)
```
We will chose All Significant Variables from AIC_lm_group_OverAll `(OverallQual,OverallCond,`1stFlrSF`,`2ndFlrSF`,TotalBsmtSF)`.
```{r}

```

### 4) 4th Group :
```{r}
group_Others    <-   as.formula(log(SalePrice) ~ MSSubClass+MSZoning+log(LotArea)+Neighborhood+HouseStyle)
pairs(group_Others,data = df_ames)

lm_group_Others <-  lm(group_Others, data = df_ames)
summary(lm_group_Others)
```

```{r}
par(mfrow = c(2, 2))
plot(lm_group_Others)
```

Analysis of Variance:
```{r}
aov(lm_group_Others)
```

Detect MultiCollinearity:
```{r}
vif(lm_group_Others)
```
We will exclude Neighbourhood  and MSZoning as It possess high MultiCollinearity.

Model selection using AIC:
```{r}
AIC_lm_group_Others <- stepAIC(lm_group_Others,direction = "backward",k=2, trace = 0, steps = 1000)
summary(lm_group_Others)
```
We will chose Most Significant Variables from AIC_lm_group_Exter Model `(LotArea,MSZoning)`

### 5) 5th Group :
```{r}
group_Garage1  <-    as.formula(log(SalePrice) ~ BedroomAbvGr+KitchenAbvGr+PoolArea+GarageArea+GarageQual)
pairs(group_Garage1, data = df_ames)

lm_group_Garage1 <-  lm(group_Garage1, data = df_ames)
summary(lm_group_Garage1)
```

```{r}
par(mfrow = c(2, 2))
plot(lm_group_Garage1)
```

Analysis of Variance:
```{r}
aov(lm_group_Garage1)
```

Detect MultiCollinearity:
```{r}
vif(lm_group_Garage1)
```

Model selection using AIC:
```{r}
AIC_lm_group_Garage <- stepAIC(lm_group_Garage1,direction = "backward",k=2, trace = 0, steps = 1000)
summary(AIC_lm_group_Garage)
```
We will chose All Significant Variables from AIC_lm_group_Garage `(BedroomAbvGr+KitchenAbvGr+GarageArea)`.


# Final Model:
```{r}
group_final <-  as.formula("log(SalePrice) ~ BsmtQual+BsmtCond+ExterQual+MasVnrArea+log(LotArea)+MSZoning+OverallQual+OverallCond+`1stFlrSF`+`2ndFlrSF`+TotalBsmtSF+BedroomAbvGr+KitchenAbvGr+GarageArea")
final_model <- lm(group_final, data = df_ames)
summary(final_model)
```


```{r}
par(mfrow = c(2, 2))
plot(final_model)
```

Analysis of Variance:
```{r}
aov(final_model)
```

Detect MultiCollinearity:
```{r}
vif(final_model)
```

Model selection using AIC:
```{r}
AIC_final_model <- stepAIC(final_model,direction = "backward",k=2, trace = 0, steps = 1000)
summary(AIC_final_model)
```

We will chose All Significant Variables from AIC_final_model `(GarageArea +`1stFlrSF`+`2ndFlrSF`+OverallQual+OverallCond+MSZoning+LotArea+ExterQual+BsmtCond+BsmtQual)`

We will Final Model after Performing AIC for Variables Detection for Significance.
```{r}
final_model1 <- as.formula("log(SalePrice) ~ GarageArea +`1stFlrSF`+`2ndFlrSF`+OverallQual+OverallCond+MSZoning+log(LotArea)+BsmtCond+BsmtQual")
final_model_lm <- lm(final_model1, data = df_ames)
summary(final_model_lm)
```

R-Squared is 85% which mean these 10 Variables id defining 85% variability of Sales Price.where Residual Standard Error is quite low as 0.1.which is Satisfactory and F-statistics is also High enough.P-value is 0 which Indicate Our Alternative Model Accuracy.

```{r}
par(mfrow=c(2,2))
plot(final_model)
```

```{r}
ols_plot_resid_fit(final_model)
ols_plot_resid_lev(final_model)
ols_plot_resid_stud(final_model)
ols_plot_resid_qq(final_model)
ols_plot_dffits(final_model)
ols_plot_cooksd_chart(final_model)
ols_plot_hadi(final_model)
ols_plot_added_variable(final_model)
ols_plot_comp_plus_resid(final_model)
```

## ANOVA for Comparing Two Models:
Ho = both model are equal there is no Significant difference
H1 = Full Model is Significantly better than final_model Model.
```{r}
ANOVA1 <- Anova(final_model_lm,full_model_lm)
summary(ANOVA1)
```
Since p-value is 0 So we can conclude that Our Full Model is more Significant than Our Final Model.

## Model Prediction:
We will Predict Sales Price with 95 Confidence.
```{r}
predict.full <- exp(predict(final_model_lm, df_test, interval = "prediction",level=0.95))
SalePrice <-predict.full[,1]

model_output <- cbind(df_test,SalePrice)
head(model_output)
```



